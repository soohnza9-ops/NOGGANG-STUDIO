import React, { useState, useEffect, useRef, useMemo } from 'react';
import Header from './components/Header';
import ScriptInput from './components/ScriptInput';
import StyleSelector from './components/StyleSelector';
import VoiceSelector from './components/VoiceSelector';
import BgmSelector from './components/BgmSelector';
import SceneCard from './components/SceneCard';
import VideoSettingsPanel from './components/VideoSettingsPanel';
import VideoExporter from './components/VideoExporter';
import AdvancedSettings from './components/AdvancedSettings';
import AssetLibrary from './components/AssetLibrary';
import { AppState, ArtStyle, Scene, SubtitleSize, SceneTimelineEntry, UserAsset, VoiceName } from './types';
import { analyzeScriptToScenes, generateSceneImage, generateSingleAudio, generateAudioBatch } from './services/geminiService';
import { Loader2, Sparkles, ChevronLeft, PlayCircle, StopCircle, CheckCircle2, Music, X, Monitor, Mic2, ImageIcon, Plus, Type, Wand2 } from 'lucide-react';
import JSZip from 'jszip';
import { Undo2, Redo2 } from 'lucide-react';
const API_KEY =
  typeof window !== "undefined"
    ? localStorage.getItem("GEMINI_API_KEY") || ""
    : "";

  const VOICE_LABEL_MAP: Record<string, string> = {
  // ì—¬ì„±
  Achernar: 'ì•„ì¼€ë¥´ë‚˜ë¥´',
  Aoede: 'ì•„ì˜¤ì´ë°',
  Autonoe: 'ì•„ìš°í† ë…¸ì—',
  Callirrhoe: 'ì¹¼ë¦¬ë¡œì—',
  Despina: 'ë°ìŠ¤í”¼ë‚˜',
  Erinome: 'ì—ë¦¬ë…¸ë©”',
  Gacrux: 'ê°€í¬ë£©ìŠ¤',
  Kore: 'ì½”ë ˆ',
  Laomedeia: 'ë¼ì˜¤ë©”ë°ì´ì•„',
  Leda: 'ë ˆë‹¤',
  Pulcherrima: 'í’€ì¼€ë¦¬ë§ˆ',
  Sulafat: 'ìˆ ë¼íŒŒíŠ¸',
  Vindemiatrix: 'ë¹ˆë°ë¯¸ì•„íŠ¸ë¦­ìŠ¤',
  Zephyr: 'ì œí”¼ë¥´',

  // ë‚¨ì„±
  Achird: 'ì•„í‚¤ë¥´ë“œ',
  Algenib: 'ì•Œì œë‹ˆë¸Œ',
  Algieba: 'ì•Œê¸°ì—ë°”',
  Alnilam: 'ì•Œë‹ëŒ',
  Charon: 'ì¹´ë¡ ',
  Enceladus: 'ì—”ì¼ˆë¼ë‘ìŠ¤',
  Fenrir: 'íœë¦´',
  Iapetus: 'ì´ì•„í˜íˆ¬ìŠ¤',
  Orus: 'ì˜¤ëŸ¬ìŠ¤',
  Puck: 'í½',
  Rasalgethi: 'ë¼ì‚´ê²Œí‹°',
  Sadachbia: 'ì‚¬ë‹¤í¬ë¹„ì•„',
  Sadaltager: 'ì‚¬ë‹¬íƒ€ê²Œë¥´',
  Schedar: 'ì…°ë‹¤ë¥´',
  Umbriel: 'ì—„ë¸Œë¦¬ì—˜',
  Zubenelgenubi: 'ì£¼ë² ë„¬ê²Œëˆ„ë¹„',
};


const formatTime = (seconds: number): string => {
  const m = Math.floor(seconds / 60);
  const s = Math.floor(seconds % 60);
  return `${m}:${s.toString().padStart(2, '0')}`;
};

const formatSrtTime = (seconds: number): string => {
  const hrs = Math.floor(seconds / 3600).toString().padStart(2, '0');
  const mins = Math.floor((seconds % 3600) / 60).toString().padStart(2, '0');
  const secs = Math.floor(seconds % 60).toString().padStart(2, '0');
  const ms = Math.floor((seconds % 1) * 1000).toString().padStart(3, '0');
  return `${hrs}:${mins}:${secs},${ms}`;
};




const formatAssetSubtitle = (
  text: string,
  subtitleSize: SubtitleSize
): string => {
  // ì—ì…‹ ì €ì¥ìš©: ì¤„ë°”ê¿ˆ â†’ ê³µë°±
  const normalized = text.replace(/\r?\n+/g, ' ').trim();

  // S ì‚¬ì´ì¦ˆë§Œ ë¬¸ì¥ ë‹¨ìœ„ ì¤„ë°”ê¿ˆ
  if (subtitleSize !== 'S') return normalized;

  // 10ì ë¯¸ë§Œì€ ì¤„ë°”ê¿ˆ ê¸ˆì§€
  if (normalized.length < 10) return normalized;

  return normalized
    .split(/(?<=[.!?])/)
    .map(s => s.trim())
    .filter(Boolean)
    .join('\n');
};


const SUBTITLE_RATIOS = {
  '16:9': { S: 0.03, M: 0.045, L: 0.055 },
  '9:16': { S: 0.05, M: 0.075, L: 0.09 }
};

const audioBufferToWav = (buffer: AudioBuffer): ArrayBuffer => {
  const numOfChan = buffer.numberOfChannels;
  const length = buffer.length * numOfChan * 2 + 44;
  const bufferArray = new ArrayBuffer(length);
  const view = new DataView(bufferArray);
  let pos = 0;
  const setUint16 = (data: number) => { view.setUint16(pos, data, true); pos += 2; };
  const setUint32 = (data: number) => { view.setUint32(pos, data, true); pos += 4; };
  setUint32(0x46464952); setUint32(length - 8); setUint32(0x45564157); setUint32(0x20746d66); setUint32(16);
  setUint16(1); setUint16(numOfChan); setUint32(buffer.sampleRate); setUint32(buffer.sampleRate * 2 * numOfChan);
  setUint16(numOfChan * 2); setUint16(16); setUint32(0x61746164); setUint32(length - pos - 4);
  for (let i = 0; i < buffer.length; i++) {
    for (let channel = 0; channel < numOfChan; channel++) {
      let sample = buffer.getChannelData(channel)[i];
      sample = Math.max(-1, Math.min(1, sample));
      sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
      view.setInt16(pos, sample, true); pos += 2;
    }
  }
  return bufferArray;
};

export const getBalancedLines = (
  ctx: CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D,
  text: string,
  maxWidth: number,
  subtitleSize: SubtitleSize
): string[] => {

  if (!text) return [];

  const t = String(text)
    .replace(/\r/g, '')
    .replace(/\s+/g, ' ')
    .trim();

  if (!t) return [];

const fullW = ctx.measureText(t).width;

const ONE_LINE_RATIO =
  subtitleSize === 'S' ? 0.85 :
  subtitleSize === 'M' ? 0.92 :
  0.98;

if (fullW <= maxWidth * ONE_LINE_RATIO) {
  return [t];
}


  // --- ë¬´ì¡°ê±´ 2ì¤„ ---
  const breaks: number[] = [];
 for (let i = 1; i < t.length; i++) {
  const prev = t[i - 1];
  const after = t.slice(i).trim();

  // âœ… ì‰¼í‘œ ë’¤ê°€ "ëª…ì‚¬ ë‚˜ì—´"ì´ë©´ ì¤„ë°”ê¿ˆ ê¸ˆì§€
  const isListComma =
    prev === ',' &&
    (
      // ì‰¼í‘œ ë’¤ê°€ í•œ ë‹¨ì–´ ëª…ì‚¬ + ë˜ ë‹¤ë¥¸ ì‰¼í‘œê°€ ë‚¨ì•„ìˆìŒ
      /^[ê°€-í£]+/.test(after) && after.includes(',')
    );

  if (
    (
      prev === ',' && !isListComma
    ) ||
    (
      prev === ' ' ||
      /[.!?â€¦)\\]]/.test(prev)
    ) &&
    !/^[ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬ì‹­]+$/.test(t.slice(0, i).trim()) &&
    !/(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ë¡œ|ì™€|ê³¼|ì˜)$/.test(t.slice(0, i).trim())
  ) {
    breaks.push(i);
  }
}


  // ëŠì„ ë° ì—†ìœ¼ë©´ ê°€ìš´ë°
  if (breaks.length === 0) {
    const mid = Math.floor(t.length / 2);
    return [
      t.slice(0, mid).trim(),
      t.slice(mid).trim()
    ];
  }

  // ê°€ì¥ í­ ì°¨ì´ê°€ ì ì€ ì§€ì  ì„ íƒ (Workerì™€ ë™ì¼)
  let best: { a: string; b: string; diff: number } | null = null;

  for (const idx of breaks) {
    const a = t.slice(0, idx).trim();
    const b = t.slice(idx).trim();
    if (!a || !b) continue;

    const diff = Math.abs(
      ctx.measureText(a).width - ctx.measureText(b).width
    );
    if (!best || diff < best.diff) best = { a, b, diff };
  }

  if (best) return [best.a, best.b];

  const cut = breaks[Math.floor(breaks.length / 2)];
  return [
    t.slice(0, cut).trim(),
    t.slice(cut).trim()
  ];
};





/** ============================
 * âœ… í‹±(í´ë¦­) ì™„í™”ìš© í›„ì²˜ë¦¬ ìœ í‹¸
 * - ì‹œì‘/ë í´ë¦­: 5~10ms í˜ì´ë“œë¡œ ì™„í™”
 * - DC offset: ì•„ì£¼ ë¯¸ì„¸í•œ í‹±/íŒ ì™„í™”
 * ============================ */
const removeDcOffsetInPlace = (buf: AudioBuffer) => {
  if (!buf || buf.numberOfChannels < 1) return;
  const ch0 = buf.getChannelData(0);
  let sum = 0;
  for (let i = 0; i < ch0.length; i++) sum += ch0[i];
  const mean = sum / Math.max(1, ch0.length);
  if (!isFinite(mean) || Math.abs(mean) < 1e-6) return;
  for (let i = 0; i < ch0.length; i++) ch0[i] = ch0[i] - mean;
};

const applyFadeInOutInPlace = (buf: AudioBuffer, fadeMs = 8) => {
  if (!buf || buf.numberOfChannels < 1) return;
  const sr = buf.sampleRate;
  const n = buf.length;
  const fade = Math.min(Math.floor((fadeMs / 1000) * sr), Math.floor(n / 2));
  if (fade <= 1) return;

  const ch0 = buf.getChannelData(0);

  // fade-in
  for (let i = 0; i < fade; i++) {
    const g = i / fade;
    ch0[i] *= g;
  }

  // fade-out
  for (let i = 0; i < fade; i++) {
    const g = (fade - i) / fade;
    ch0[n - 1 - i] *= g;
  }
};

const postProcessSceneAudioInPlace = (buf: AudioBuffer | null | undefined) => {
  if (!buf) return;
  try {
    removeDcOffsetInPlace(buf);
    // âŒ ì”¬ ë‹¨ìœ„ í˜ì´ë“œ ì™„ì „ ì œê±° (crossfade ê¹¨ì§ ì›ì¸)
  } catch {}
};


const trimSilenceInPlace = (
  buf: AudioBuffer,
  threshold = 0.0025,
  minSilenceMs = 0
) => {
  const ch = buf.getChannelData(0);
  const sr = buf.sampleRate;

  let start = 0;
  while (start < ch.length && Math.abs(ch[start]) < threshold) start++;

  let end = ch.length - 1;
  while (end > start && Math.abs(ch[end]) < threshold) end--;

  if (start >= end) return buf;

  const newLen = end - start + 1;
  const trimmed = new AudioContext({ sampleRate: sr }).createBuffer(1, newLen, sr);
  trimmed.getChannelData(0).set(ch.slice(start, end + 1));

  return trimmed;
};





// âœ… íƒ€ì„ë¼ì¸ì—ì„œ ì´ë¯¸ 0.3ì´ˆ gapì„ ì“°ê³  ìˆìœ¼ë¯€ë¡œ, fullSpeechAudioBufferì—ë„ ë™ì¼í•˜ê²Œ ë„£ì–´ ì‹±í¬/í´ë¦­ ë¬¸ì œ ì™„í™”
const AUDIO_SR = 24000;
const SCENE_GAP_SECONDS = 0;
const SCENE_GAP_SAMPLES = Math.round(SCENE_GAP_SECONDS * AUDIO_SR);

const App: React.FC = () => {
    const API_KEY = useMemo(() => {
    return localStorage.getItem("GEMINI_API_KEY") || "";
  }, []);
    const [viewMode, setViewMode] = useState<'setup' | 'workspace'>('setup');
  const sceneRefs = useRef<Record<string, HTMLDivElement | null>>({});

  const [previewWidth, setPreviewWidth] = useState(0);
  


  const [state, setState] = useState<AppState>({
    script: '', scenes: [], metadata: null, selectedStyle: ArtStyle.REALISTIC, selectedVoice: 'Achird',
    voiceSpeed: 1.0, voicePitch: 0.0,
    voiceStylePrompt: 'ì‹ ë¢°ê° ìˆëŠ” ì „ë¬¸ê°€ ìŠ¤íƒ€ì¼.',
    bgmUrl: null, bgmVolume: 0.05,
    referenceImage: undefined, userAssets: [], isAnalyzing: false, error: null, characterPrompt: '', atmospherePrompt: '',
    videoSettings: { aspectRatio: '16:9', subtitleSize: 'S', subtitlePosition: 5, subtitleColor: '#FFFFFF', showSubtitleBox: true },
    skipInitialImageGen: false
  });

 

  

  const [isGenerating, setIsGenerating] = useState(false);
  const [isImageBatchRunningState, setIsImageBatchRunningState] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
const [activeBreathKey, setActiveBreathKey] = useState<string | null>(null);
const [breathEditText, setBreathEditText] = useState<string>("");
const [isBreathEditing, setIsBreathEditing] = useState<boolean>(false);
const breathEditTextRef = useRef<string>("");
const [isExporting, setIsExporting] = useState(false);
const [currentTime, setCurrentTime] = useState(0);
const [isZipping, setIsZipping] = useState(false);
const [zipProgress, setZipProgress] = useState(0);
const [exportProgress, setExportProgress] = useState(0);
const [isConfirmOpen, setIsConfirmOpen] = useState(false);
const [isVideoLoading, setIsVideoLoading] = useState(false);
const [statusLog, setStatusLog] = useState<string>("");
const audioCtxRef = useRef<AudioContext | null>(null);
const audioSourceNodeRef = useRef<AudioBufferSourceNode | null>(null);
const requestRef = useRef<number>(0);
const isBatchRunning = useRef(false);
const bgmAudioRef = useRef<HTMLAudioElement | null>(null);
const lastBgmUrlRef = useRef<string | null>(null);
const videoARef = useRef<HTMLVideoElement | null>(null);
const videoBRef = useRef<HTMLVideoElement | null>(null);
const [activeVideo, setActiveVideo] = useState<'A' | 'B'>('A');
const freezeOnEndRef = useRef(false);
const didInitSubtitlePositionRef = useRef(false);
const previewTimeRef = useRef(0); // âœ… ë¯¸ë¦¬ë³´ê¸° ì‹¤ì œ ì‹œê°„ ê¸°ì¤€ (ë Œë”ë§/ë¹„ë””ì˜¤ìš©)
const playTokenRef = useRef(0);
const currentTimeRef = useRef(0);

  // ============================
// âœ… Undo/Redo (ì´ˆê¸° ìƒì„± ë³´í˜¸ í¬í•¨)
// ============================
type SceneSnapshot = { scenes: Scene[] };
const MAX_HISTORY = 30;

const historyRef = useRef<SceneSnapshot[]>([]);
const redoRef = useRef<SceneSnapshot[]>([]);
const baselineReadyRef = useRef(false);     // âœ… ì´ˆê¸° ìƒì„±(ë¶„ì„) ëë‚œ ë’¤ë¶€í„° Undo í—ˆìš©
const isInitialAutoGenRef = useRef(false);  // âœ… ì´ˆê¸° ìë™ ìƒì„± ì¤‘ì—ëŠ” ê¸°ë¡ ê¸ˆì§€

const [historyCount, setHistoryCount] = useState(0);
const [redoCount, setRedoCount] = useState(0);





// setStateê°€ ë¹„ë™ê¸°ë¼, "í˜„ì¬ state"ë¥¼ ì½ê¸° ìœ„í•œ ref
const stateRef = useRef<AppState>(state);
useEffect(() => { stateRef.current = state; }, [state]);



const cloneScenes = (scenes: Scene[]) => scenes.map(s => ({ ...s }));

const getSnapshot = (): SceneSnapshot => {
  const st = stateRef.current;
  return { scenes: cloneScenes(st.scenes || []) };
};

const resetHistory = () => {
  historyRef.current = [];
  redoRef.current = [];
  setHistoryCount(0);
  setRedoCount(0);
};

const pushToHistory = () => {
  // âœ… ì´ˆê¸° ë¶„ì„ ì™„ë£Œ ì „ì—ëŠ” ê¸°ë¡ ì•ˆ í•¨
  if (!baselineReadyRef.current) return;

  const snap = getSnapshot();
  if (!snap.scenes || snap.scenes.length === 0) return;

  historyRef.current = [...historyRef.current, snap];

  if (historyRef.current.length > MAX_HISTORY) {
    historyRef.current = historyRef.current.slice(-MAX_HISTORY);
  }

  redoRef.current = [];
  setHistoryCount(historyRef.current.length);
  setRedoCount(0);
};


const undo = () => {
  if (historyRef.current.length === 0) return;
  if (isPlaying) stopPreview();

  const current = getSnapshot();
  const prev = historyRef.current[historyRef.current.length - 1];

  historyRef.current = historyRef.current.slice(0, -1);
  redoRef.current = [current, ...redoRef.current];

  setHistoryCount(historyRef.current.length);
  setRedoCount(redoRef.current.length);

  setState(p => ({ ...p, scenes: cloneScenes(prev.scenes) }));
};

const redo = () => {
  if (redoRef.current.length === 0) return;
  if (isPlaying) stopPreview();

  const current = getSnapshot();
  const next = redoRef.current[0];

  redoRef.current = redoRef.current.slice(1);
  historyRef.current = [...historyRef.current, current];

  setHistoryCount(historyRef.current.length);
  setRedoCount(redoRef.current.length);

  setState(p => ({ ...p, scenes: cloneScenes(next.scenes) }));
};
useEffect(() => {
  const onKeyDown = (e: KeyboardEvent) => {
    if (viewMode !== 'workspace') return;

    const tag = (document.activeElement?.tagName || '').toUpperCase();
    const isTyping =
      tag === 'TEXTAREA' ||
      tag === 'INPUT' ||
      (document.activeElement as any)?.isContentEditable;
    if (isTyping) return;

    if ((e.ctrlKey || e.metaKey) && e.code === 'KeyZ') {
      e.preventDefault();
      if (e.shiftKey) redo();
      else undo();
    }
  };

  window.addEventListener('keydown', onKeyDown);
  return () => window.removeEventListener('keydown', onKeyDown);
}, [viewMode]);




  /** âœ… fullSpeechAudioBuffer: íƒ€ì„ë¼ì¸ gap(0.3s)ì„ ì‹¤ì œ ë²„í¼ì—ë„ ì‚½ì… */
  const fullSpeechAudioBuffer = useMemo(() => {
    const spoken = state.scenes.filter(s => !s.isHeader);
    const hasAny = spoken.some(s => !!s.audioBuffer);
    if (!hasAny && spoken.length === 0) return null;

    const ctx = new AudioContext({ sampleRate: AUDIO_SR });

    // ì´ ìƒ˜í”Œ: (ê° scene audio/estimate) + (scene ì‚¬ì´ gap)
    const totalSamples = spoken.reduce((acc, s, idx) => {
      const sceneSamples = s.audioBuffer
        ? s.audioBuffer.length
        : Math.round((s.estimatedDurationSeconds || 5) * AUDIO_SR);

      const gap = (idx < spoken.length - 1) ? SCENE_GAP_SAMPLES : 0;
      return acc + sceneSamples + gap;
    }, 0);

    if (totalSamples <= 0) return null;

    const finalBuffer = ctx.createBuffer(1, totalSamples, AUDIO_SR);
    const out = finalBuffer.getChannelData(0);

    let offset = 0;
    spoken.forEach((s, idx) => {
      const isLast = idx === spoken.length - 1;

            if (s.audioBuffer) {
        // ë³µì‚¬
        const src = s.audioBuffer.getChannelData(0);

        const start = offset;

        // 1) ì˜¤ë””ì˜¤ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì¶”ê°€ í˜ì´ë“œ/í¬ë¡œìŠ¤í˜ì´ë“œ ì œê±°)
        out.set(src, start);

        offset += src.length;

      } else {
        offset += Math.round((s.estimatedDurationSeconds || 5) * AUDIO_SR);
      }


      // âœ… gap ì‚½ì… (ê¸°ë³¸ê°’ 0ì´ë¼ out.set í•„ìš” ì—†ìŒ)
      if (!isLast) offset += SCENE_GAP_SAMPLES;
    });
applyFadeInOutInPlace(finalBuffer, 12);
return finalBuffer;
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [state.scenes]);

  const sceneTimeline: SceneTimelineEntry[] = useMemo(() => {
    let cumV = 0; let cumA = 0;
    const spokenCount = state.scenes.filter(s => !s.isHeader).length;
    let spokenIdx = 0;

    return state.scenes.map((s) => {
      let dur = s.isHeader ? 0.001 : (s.audioBuffer?.duration ?? s.estimatedDurationSeconds ?? 5.0);

      // âœ… ì˜ìƒ/ìë§‰ íƒ€ì„ë¼ì¸ì—ì„œë„ gapì„ ì¤€ë‹¤
      if (!s.isHeader) {
        spokenIdx++;
        if (spokenIdx < spokenCount) {
          dur += SCENE_GAP_SECONDS;
        }
      }

      const entry: SceneTimelineEntry = {
        start: cumV,
        end: cumV + dur,
        scene: s,
        duration: dur,
        audioOffsetInFullBuffer: s.isHeader ? undefined : cumA
      };

      cumV += dur;
      if (!s.isHeader) cumA += dur;
      return entry;
    });
  }, [state.scenes]);

  const stats = useMemo(() => {
  const spoken = state.scenes.filter(s => !s.isHeader);

  const hasError = spoken.some(s => s.status === 'error');
  const hasMissingImage = spoken.some(
    s => !s.imageUrl || s.imageUrl.startsWith('data:image/svg')
  );

  const totalItems = spoken.length * 2;

  const completedItems = spoken.reduce((acc, s) => {
    let count = 0;
    if (s.audioBuffer) count++;

    // â— ì‹¤íŒ¨í•œ ì´ë¯¸ì§€ëŠ” ì ˆëŒ€ ì™„ë£Œë¡œ ì¹˜ì§€ ì•ŠìŒ
    if (
      s.imageUrl &&
      !s.imageUrl.startsWith('data:image/svg') &&
      s.status !== 'error'
    ) {
      count++;
    }

    return acc + count;
  }, 0);

  const progress =
    totalItems === 0 ? 0 : Math.floor((completedItems / totalItems) * 100);

  const isAllReady =
    progress === 100 &&
    spoken.length > 0 &&
    !hasError &&
    !hasMissingImage;

  return {
    progress,
    isAllReady,
    readyAudio: spoken.filter(s => s.audioBuffer).length,
    failed: spoken.filter(s => s.status === 'error').length,
    hasMissingImage,
    hasError
  };
}, [state.scenes]);


  const totalDuration = useMemo(() => sceneTimeline.length ? sceneTimeline[sceneTimeline.length - 1].end : 0, [sceneTimeline]);
  const currentTimelineEntry = useMemo(() => {
  // 1ï¸âƒ£ ì •ìƒ ì¬ìƒ ì¤‘
  const active = sceneTimeline.find(e =>
    !e.scene.isHeader &&
    currentTime >= e.start &&
    currentTime < e.end
  );
  if (active) return active;

  // 2ï¸âƒ£ ì¬ìƒ ëì— ë„ë‹¬í•œ ê²½ìš° â†’ ë§ˆì§€ë§‰ ì”¬ ìœ ì§€
  if (currentTime >= totalDuration && sceneTimeline.length > 0) {
    return [...sceneTimeline]
      .reverse()
      .find(e => !e.scene.isHeader);
  }

  // 3ï¸âƒ£ ê·¸ ì™¸ (ì´ˆê¸° ìƒíƒœ ë“±)
  return sceneTimeline.find(e => !e.scene.isHeader);
}, [sceneTimeline, currentTime, totalDuration]);

  const currentScene = currentTimelineEntry?.scene;

const lastAutoScrollSceneIdRef = useRef<string | null>(null);

useEffect(() => {
  if (viewMode !== 'workspace') return;
  if (!currentScene?.id) return;

  if (lastAutoScrollSceneIdRef.current === currentScene.id) return;
  lastAutoScrollSceneIdRef.current = currentScene.id;

  const el = sceneRefs.current[currentScene.id];
  if (!el) return;

  el.scrollIntoView({
    behavior: 'smooth',
    block: 'center'
  });
}, [viewMode, currentScene?.id]);


  const currentZoomScale = useMemo(() => {
  if (!currentTimelineEntry) return 1.0;

  const sceneStart = currentTimelineEntry.start;
  const sceneEnd = currentTimelineEntry.end;
  const sceneDuration = Math.max(0.001, sceneEnd - sceneStart);

  const elapsed = Math.max(0, Math.min(sceneDuration, currentTime - sceneStart));
  const p = elapsed / sceneDuration; // 0~1

  const entryIndex = sceneTimeline.indexOf(currentTimelineEntry);
  const isZoomOut = entryIndex % 2 !== 0;

  const lerp = (a: number, b: number, t: number) => a + (b - a) * t;

  return isZoomOut
    ? lerp(1.2, 1.0, p)
    : lerp(1.0, 1.2, p);
}, [currentTimelineEntry, currentTime, sceneTimeline]);


  const currentHeader = useMemo(() => {
    let lastHeader = '';
    for (const entry of sceneTimeline) {
      if (entry.start <= currentTime) {
        if (entry.scene.isHeader) lastHeader = entry.scene.subtitle;
      } else break;
    }
    return lastHeader;
  }, [sceneTimeline, currentTime]);
useEffect(() => {
  if (viewMode !== 'workspace') return;

  const el = document.getElementById('preview-container');
  if (!el) return;

  const update = () => {
    const w = Math.floor(el.getBoundingClientRect().width);
    if (w > 0) setPreviewWidth(w);
  };

  update();

  const ro = new ResizeObserver(() => update());
  ro.observe(el);

  return () => ro.disconnect();
}, [viewMode, state.videoSettings.aspectRatio]);

const subtitleLines = useMemo(() => {
  if (!currentScene || currentScene.isHeader) return [];
  if (previewWidth <= 0) return [currentScene.subtitle];

  const canvas = document.createElement('canvas');
  const ctx = canvas.getContext('2d')!;

  const ratioConfig = SUBTITLE_RATIOS[state.videoSettings.aspectRatio as '16:9' | '9:16'];
  const M_ONLY_SCALE = 0.90;

  const base = previewWidth * ratioConfig[state.videoSettings.subtitleSize as SubtitleSize];
  const fontSize =
    state.videoSettings.subtitleSize === 'M'
      ? base * M_ONLY_SCALE
      : base;

  ctx.font = `900 ${fontSize}px "Noto Sans KR"`;

  const limitRatio = state.videoSettings.aspectRatio === '9:16' ? 0.90 : 0.95;
  const maxWidth = previewWidth * limitRatio;

  return getBalancedLines(ctx, currentScene.subtitle, maxWidth, state.videoSettings.subtitleSize);
}, [currentScene, previewWidth, state.videoSettings.subtitleSize, state.videoSettings.aspectRatio]);


useEffect(() => {
  const vid =
    activeVideo === 'A'
      ? videoARef.current
      : videoBRef.current;

  if (!vid) return;

  // ë¹„ë””ì˜¤ ì¥ë©´ ì•„ë‹ˆë©´ ë©ˆì¶”ê¸°
  if (currentScene?.userAssetType !== 'video') {
    try { vid.pause(); } catch {}
    return;
  }

  if (!currentTimelineEntry) return;

  const sceneDuration = currentTimelineEntry.end - currentTimelineEntry.start;
  if (sceneDuration <= 0) return;

  const applyRateAndMaybePlay = async () => {
    // ë©”íƒ€ë°ì´í„°/ì²« í”„ë ˆì„ ë¡œë”© ì „ì´ë©´ durationì´ NaNì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì—¬ê¸°ì„œ ì¬í™•ì¸
    if (isFinite(vid.duration) && vid.duration > 0) {
      const desired = vid.duration / sceneDuration;

      // âœ… ë„ˆë¬´ ëŠë¦¬ê²Œ(0.25 ê°™ì€ ê°’) ëŠ˜ë¦¬ë©´ ëŠê¹€ì´ ì‹¬í•´ì§ â†’ í•˜í•œì„ ì˜¬ë¦¼
      const rate = Math.max(0.5, Math.min(desired, 4.0));
      vid.playbackRate = rate;

      // âœ… â€œì–µì§€ë¡œ ëŠ˜ë¦° ì¥ë©´â€(desired < 0.5)ì€ ëì—ì„œ ë©ˆì¶° ë§ˆì§€ë§‰ í”„ë ˆì„ ê³ ì •
      freezeOnEndRef.current = desired < 0.5;
    } else {
      // duration ì•„ì§ ëª¨ë¥´ë©´ ì„ì‹œë¡œ ì •ìƒê°’
      vid.playbackRate = 1.0;
      freezeOnEndRef.current = false;
    }

    if (isPlaying) {
      try {
        await vid.play();
      } catch {}
    } else {
      try { vid.pause(); } catch {}
    }
  };

  // âœ… srcê°€ ë°”ë€Œë©´ readyStateê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë²¤íŠ¸ë¡œ ì¬ì‹œë„
  const onLoadedMetadata = () => { applyRateAndMaybePlay(); };
  const onLoadedData = () => { applyRateAndMaybePlay(); };

  // âœ… ë í”„ë ˆì„ ê³ ì • (desired < 0.5ì¼ ë•Œë§Œ)
  const onTimeUpdate = () => {
    if (!freezeOnEndRef.current) return;
    if (!isFinite(vid.duration) || vid.duration <= 0) return;

    // ë§ˆì§€ë§‰ ê·¼ì²˜ì—ì„œ ì •ì§€ + ë§ˆì§€ë§‰ í”„ë ˆì„ ìœ ì§€
    if (vid.currentTime >= vid.duration - 0.06) {
      try { vid.pause(); } catch {}
      try { vid.currentTime = Math.max(0, vid.duration - 0.04); } catch {}
    }
  };

  vid.addEventListener('loadedmetadata', onLoadedMetadata);
  vid.addEventListener('loadeddata', onLoadedData);
  vid.addEventListener('timeupdate', onTimeUpdate);

  // âœ… ì¦‰ì‹œ í•œ ë²ˆ ì‹œë„ (ì´ë¯¸ ë¡œë“œëœ ìƒíƒœë©´ ë°”ë¡œ ì ìš©)
  applyRateAndMaybePlay();

  return () => {
    vid.removeEventListener('loadedmetadata', onLoadedMetadata);
    vid.removeEventListener('loadeddata', onLoadedData);
    vid.removeEventListener('timeupdate', onTimeUpdate);
  };
}, [isPlaying, currentScene?.imageUrl, currentTimelineEntry, activeVideo]);

useEffect(() => {
  if (!currentScene) return;
  if (currentScene.userAssetType !== 'video') return;
  if (!currentScene.imageUrl) return;

  const nextVideo =
    activeVideo === 'A'
      ? videoBRef.current
      : videoARef.current;

  const currentVideo =
    activeVideo === 'A'
      ? videoARef.current
      : videoBRef.current;

  if (!nextVideo || !currentVideo) return;

  let cancelled = false;

  // 1ï¸âƒ£ ë‹¤ìŒ ë¹„ë””ì˜¤ ì¤€ë¹„
  nextVideo.pause();
  nextVideo.src = currentScene.imageUrl;
  nextVideo.currentTime = 0;
  nextVideo.load();

  const onReady = async () => {
    if (cancelled) return;

    try {
      await nextVideo.play();
    } catch {}

    // 2ï¸âƒ£ "ì²« í”„ë ˆì„ì´ ì‹¤ì œë¡œ ê·¸ë ¤ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°"
    if ('requestVideoFrameCallback' in nextVideo) {
      (nextVideo as any).requestVideoFrameCallback(() => {
        if (cancelled) return;

        // ğŸ”¥ ì—¬ê¸°ì„œë§Œ ìŠ¤ì™‘
        setActiveVideo(prev => (prev === 'A' ? 'B' : 'A'));

        try { currentVideo.pause(); } catch {}
      });
    } else {
      // fallback (êµ¬í˜• ë¸Œë¼ìš°ì €)
      requestAnimationFrame(() => {
        if (cancelled) return;
        setActiveVideo(prev => (prev === 'A' ? 'B' : 'A'));
        try { currentVideo.pause(); } catch {}
      });
    }
  };

  nextVideo.addEventListener('canplay', onReady, { once: true });

  return () => {
    cancelled = true;
    nextVideo.removeEventListener('canplay', onReady);
  };
}, [currentScene?.imageUrl]);





  useEffect(() => {
    if (state.bgmUrl) {
      if (state.bgmUrl !== lastBgmUrlRef.current) {
        if (bgmAudioRef.current) { bgmAudioRef.current.pause(); bgmAudioRef.current.src = ""; }
        bgmAudioRef.current = new Audio(state.bgmUrl);
        bgmAudioRef.current.loop = true;
        lastBgmUrlRef.current = state.bgmUrl;
      }
      if (bgmAudioRef.current) {
        bgmAudioRef.current.volume = state.bgmVolume;
        if (isPlaying && bgmAudioRef.current.paused) {
          const bgmDuration = bgmAudioRef.current.duration;
          if (bgmDuration > 0) bgmAudioRef.current.currentTime = currentTime % bgmDuration;
          bgmAudioRef.current.play().catch(() => {});
        } else if (!isPlaying && !bgmAudioRef.current.paused) {
          bgmAudioRef.current.pause();
        }
      }
    } else if (bgmAudioRef.current) {
      bgmAudioRef.current.pause();
      bgmAudioRef.current = null;
      lastBgmUrlRef.current = null;
    }
  }, [state.bgmUrl, state.bgmVolume, isPlaying]);

  const updateSubtitle = (id: string, sub: string) => {
    setState(p => ({ ...p, scenes: p.scenes.map(sc => sc.id === id ? { ...sc, subtitle: sub } : sc) }));
  };
const updateBreathGroupSubtitle = (breathKey: string, text: string) => {
  const normalized = String(text ?? "")
    .replace(/\r/g, "")
    .trim();

  // 1) í¸ì§‘ì°½ í…ìŠ¤íŠ¸ ìœ ì§€ (ì¦‰ì‹œ ë°˜ì˜ìš© ref í¬í•¨)
  breathEditTextRef.current = normalized;
  setBreathEditText(normalized);

  // 2) ì‹¤ì œ í™”ë©´ ìë§‰(Scene.subtitle)ë„ ê°±ì‹ 
  setState(p => {
    const groupScenes = p.scenes.filter(
      s => !s.isHeader && String((s as any).breathId ?? "") === breathKey
    );

    if (groupScenes.length === 0) return p;

    const lines = normalized
      .split("\n")
      .map(v => v.trim())
      .filter(Boolean);

    // lines ìˆ˜ê°€ ì”¬ ìˆ˜ì™€ ê°™ìœ¼ë©´ 1:1ë¡œ ë°°ë¶„, ì•„ë‹ˆë©´ ì „ì²´ë¥¼ ë™ì¼í•˜ê²Œ ì ìš©
    const usePerScene = lines.length === groupScenes.length;

    const idToText = new Map<string, string>();
    groupScenes.forEach((gs, idx) => {
      idToText.set(gs.id, usePerScene ? lines[idx] : normalized);
    });

    return {
      ...p,
      scenes: p.scenes.map(sc =>
        idToText.has(sc.id)
          ? { ...sc, subtitle: idToText.get(sc.id)! }
          : sc
      )
    };
  });
};



  const updateVisualPrompt = (id: string, vp: string) => {
    setState(p => ({ ...p, scenes: p.scenes.map(sc => sc.id === id ? { ...sc, visualPrompt: vp } : sc) }));
  };

  const processSingleAsset = async (sceneId: string, type: 'image' | 'audio') => {
  pushToHistory(); // âœ… ì‚¬ìš©ì ì¬ìƒì„± ì§ì „ ê¸°ë¡
const audioCtx = new AudioContext({ sampleRate: AUDIO_SR });
try {

  if (audioCtx.state === 'suspended') {
    await audioCtx.resume();
  }

  const targetScene = state.scenes.find(s => s.id === sceneId);
  if (!targetScene) return;

  setState(p => ({
    ...p,
    scenes: p.scenes.map(sc =>
      sc.id === sceneId ? { ...sc, status: 'generating' } : sc
    )
  }));

  if (type === 'audio') {
  if (!API_KEY) throw new Error("API í‚¤ ì—†ìŒ");

  let audioBuf = await generateSingleAudio(
    API_KEY,
    targetScene,
    state.selectedVoice,
    state.voiceSpeed,
    state.voicePitch,
    audioCtx
  );

audioBuf = trimSilenceInPlace(audioBuf);

  setState(p => ({
    ...p,
    scenes: p.scenes.map(sc =>
      sc.id === sceneId
        ? { ...sc, audioBuffer: audioBuf, status: 'completed' }
        : sc
    )
  }));
}

      
      
      
      else {

const imgUrl = await generateSceneImage(
  API_KEY!,
  targetScene.visualPrompt,
  state.selectedStyle,
  state.videoSettings.aspectRatio,
  state.referenceImage,
  state.characterPrompt,
  state.atmospherePrompt
);


        setState(p => ({
          ...p,
          scenes: p.scenes.map(sc => sc.id === sceneId ? {
            ...sc,
            imageUrl: imgUrl,
            status: 'completed',
            isUserAsset: false,
            userAssetType: 'image'
          } : sc)
        }));
      }
    } catch (e: any) {
      setState(p => ({ ...p, scenes: p.scenes.map(sc => sc.id === sceneId ? { ...sc, status: 'error', errorMessage: e.message } : sc) }));
    } finally {
      audioCtx.close();
    }
  };
const processBreathGroupAudio = async (breathKey: string) => {
  if (!API_KEY) return;

  const breathIdNum = Number(breathKey);
  if (!Number.isFinite(breathIdNum)) return;

  const targetScenes = stateRef.current.scenes.filter(
    s => !s.isHeader && String((s as any).breathId ?? "") === breathKey
  );

  if (targetScenes.length === 0) return;

  setState(p => ({
    ...p,
    scenes: p.scenes.map(sc =>
      targetScenes.some(t => t.id === sc.id)
        ? { ...sc, status: 'generating', errorMessage: undefined }
        : sc
    )
  }));

  const audioCtx = new AudioContext({ sampleRate: AUDIO_SR });

  try {
    if (audioCtx.state === 'suspended') await audioCtx.resume();
    if (targetScenes.length === 0) return;

    const override = String(breathEditTextRef.current || "")
  .replace(/\r?\n+/g, " ")
  .replace(/\s+/g, " ")
  .trim();


    await generateAudioBatch(
      API_KEY!,
      targetScenes,
      stateRef.current.selectedVoice,
      stateRef.current.voiceSpeed,
      stateRef.current.voicePitch,
      audioCtx,
(id, buffer) => {
  setState(p => ({
          ...p,
          scenes: p.scenes.map(sc =>
            sc.id === id
              ? {
                  ...sc,
                  audioBuffer: buffer,
                  status: sc.imageUrl ? 'completed' : 'generating',
                  errorMessage: undefined
                }
              : sc
          )
        }));
      },
      (id, error) => {
        setState(p => ({
          ...p,
          scenes: p.scenes.map(sc =>
            sc.id === id ? { ...sc, status: 'error', errorMessage: error } : sc
          )
        }));
      },
      override
    );
  } finally {
    try { await audioCtx.close(); } catch {}
  }
};


const processFullBatch = async (type: 'image' | 'audio' | 'all' = 'all') => {
  
  if (isBatchRunning.current && type === 'all') return;
  if (isImageBatchRunningState && type === 'image') return;
  

  if (type === 'image') setIsImageBatchRunningState(true);

  isBatchRunning.current = true;
  setIsGenerating(true);
  setStatusLog(type === 'audio' ? "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘..." : "ì—ì…‹ ìƒì„± ì¤‘...");

  if (!API_KEY) throw new Error("API í‚¤ ì—†ìŒ");

  const audioCtx = new AudioContext({ sampleRate: AUDIO_SR });
  const processingImageIds = new Set<string>();

  try {
    const latestScenes = await new Promise<Scene[]>(r => setState(p => { r(p.scenes); return p; }));

   const audioPromise = (async () => {
  if (type === 'image') return;

  const pendingAudio = latestScenes.filter(s => !s.isHeader && !s.audioBuffer);
  if (pendingAudio.length === 0) return;

  // ğŸ”´ í•µì‹¬: ì‚¬ìš©ì ì œìŠ¤ì²˜ ì´í›„ AudioContext ê°•ì œ resume
  if (audioCtx.state === 'suspended') {
    await audioCtx.resume();
  }

  await generateAudioBatch(
    API_KEY!,
    pendingAudio,
    state.selectedVoice,
    state.voiceSpeed,
    state.voicePitch,
    audioCtx,
    (id, buffer) => {
      postProcessSceneAudioInPlace(buffer);
      setState(p => ({
        ...p,
        scenes: p.scenes.map(sc => {
          if (sc.id !== id) return sc;
          return {
            ...sc,
            audioBuffer: buffer,
            status: sc.imageUrl ? 'completed' : 'generating',
            errorMessage: undefined
          };
        })
      }));
    },
    (id, error) => {
      setState(p => ({
        ...p,
        scenes: p.scenes.map(sc =>
          sc.id === id
            ? { ...sc, status: 'generating', errorMessage: error }
            : sc
        )
      }));
    }
  );
})();


    const imagePromise = Promise.all(Array(12).fill(null).map(async () => {
      if (type === 'audio') return;

      let safety = 0;

      while (true) {
        if (safety++ > 200) break;

        const currentScenes = await new Promise<Scene[]>(r => setState(p => { r(p.scenes); return p; }));
        const s = currentScenes.find(sc =>
          !sc.isHeader &&
          (!sc.imageUrl || sc.imageUrl.startsWith('data:image/svg')) &&
          !processingImageIds.has(sc.id)
        );

        if (!s) break;

        processingImageIds.add(s.id);
        
        setState(p => ({
  ...p,
  scenes: p.scenes.map(sc =>
    sc.id === s.id
      ? { ...sc, status: 'generating', errorMessage: undefined }
      : sc
  )
}));

try {
  const imgUrl = await generateSceneImage(
    API_KEY,
    s.visualPrompt,
    state.selectedStyle,
    state.videoSettings.aspectRatio,
    state.referenceImage,
    state.characterPrompt,
    state.atmospherePrompt
  );

  setState(p => ({
    ...p,
    scenes: p.scenes.map(sc => {
      if (sc.id !== s.id) return sc;
      return {
        ...sc,
        imageUrl: imgUrl,
        status: sc.audioBuffer ? 'completed' : 'generating',
        errorMessage: undefined
      };
    })
  }));
} catch (e: any) {
  // âŒ ì¦‰ì‹œ errorë¡œ ë°•ì§€ ì•ŠìŒ
  setState(p => ({
    ...p,
    scenes: p.scenes.map(sc =>
      sc.id === s.id
        ? { ...sc, status: 'generating', errorMessage: e.message }
        : sc
    )
  }));
}

        finally {
          processingImageIds.delete(s.id);
        }
      }
    }));

    await Promise.all([audioPromise, imagePromise]);
  } finally {
    setIsGenerating(false);
    setIsImageBatchRunningState(false);
    isBatchRunning.current = false;
    setStatusLog("ì™„ë£Œ");
    audioCtx.close();
  }
};

const handleStartAnalysis = async () => {
  setIsConfirmOpen(false);
  setState(p => ({ ...p, isAnalyzing: true }));

  try {
    const res = await analyzeScriptToScenes(
      API_KEY!,
      state.script,
      `${state.characterPrompt}. ${state.atmospherePrompt}`,
      state.videoSettings.subtitleSize,
      state.videoSettings.aspectRatio
    );

    let expandedAssets: UserAsset[] = [];
    if (!state.skipInitialImageGen) {
      state.userAssets.forEach(asset => expandedAssets.push(asset));
    }
    expandedAssets.sort(() => Math.random() - 0.5);

    const processed = res.scenes.map(s => {
      if (s.isHeader) {
        return {
          ...s,
          status: 'completed' as const,
          visualPrompt: "ì†Œì œëª©",
          imageUrl: `data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIi8+`
        };
      }
      return { ...s, status: 'pending' as const };
    });

    setState(p => ({
      ...p,
      scenes: processed,
      metadata: res.metadata,
      isAnalyzing: false
    }));

    setViewMode('workspace');
    window.scrollTo(0, 0);
// ğŸ”¥ ë¯¸ë¦¬ë³´ê¸° ì´ˆê¸° ìƒíƒœ ê°•ì œ
previewTimeRef.current = 0;
currentTimeRef.current = 0; 
setCurrentTime(0);

// ğŸ”¥ ì²« ì¥ë©´ ì„ íƒ ìƒíƒœ
requestAnimationFrame(() => {
  handleSeek(0);
});

    baselineReadyRef.current = true;
    resetHistory();

    isBatchRunning.current = false;
if (!state.skipInitialImageGen) {
  processFullBatch('image');
}

processFullBatch('audio');


  } catch (e: any) {
    setState(p => ({
      ...p,
      isAnalyzing: false,
      error: e?.message || "ë¶„ì„ ì‹¤íŒ¨"
    }));
  }
};






  const handleManualImageUpload = (
  id: string,
  base64: string,
  type: 'image' | 'video' = 'image'
) => {
  // âœ… ë¨¼ì € íˆìŠ¤í† ë¦¬ ê¸°ë¡
  pushToHistory();

  setState(p => ({
    ...p,
    scenes: p.scenes.map(s =>
      s.id === id
        ? {
            ...s,
            imageUrl: base64,
            status: 'completed',
            isUserAsset: true,
            userAssetType: type
          }
        : s
    )
  }));
};


const handleDownloadAllAssets = async () => {
  if (isZipping) return;
  setIsZipping(true);
  setZipProgress(0);

  const zip = new JSZip();
  const imagesFolder = zip.folder("images");
  const audioFolder = zip.folder("audio");
  const srtFolder = zip.folder("subtitles");
  const spokenScenes = state.scenes.filter(s => !s.isHeader);
  const total = spokenScenes.length;

  try {
    const srtEntries: string[] = [];
    let cursor = 0;

    for (let i = 0; i < total; i++) {
      const scene = spokenScenes[i];
      const sceneNum = (i + 1).toString().padStart(3, '0');

      if (!scene.audioBuffer) {
        setZipProgress(Math.round(((i + 1) / total) * 100));
        continue;
      }

      const duration = scene.audioBuffer.duration;
      const start = cursor;
      const end = cursor + duration;

      const assetSubtitle = formatAssetSubtitle(
        scene.subtitle,
        state.videoSettings.subtitleSize
      );

      // âœ… ì „ì²´ SRTìš© ì—”íŠ¸ë¦¬ë§Œ ëˆ„ì  (ì”¬ë³„ SRT ì €ì¥ì€ í•˜ì§€ ì•ŠìŒ)
      srtEntries.push(
        `${i + 1}\n` +
        `${formatSrtTime(start)} --> ${formatSrtTime(end)}\n` +
        `${assetSubtitle}\n`
      );

      // âœ… ì´ë¯¸ì§€/ì˜ìƒì€ ê·¸ëŒ€ë¡œ ì €ì¥
      if (scene.imageUrl) {
        try {
          const res = await fetch(scene.imageUrl);
          const blob = await res.blob();
          const ext = scene.userAssetType === 'video' ? 'mp4' : 'png';
          imagesFolder?.file(`scene_${sceneNum}.${ext}`, blob);
        } catch {}
      }

      // âŒ ì”¬ë³„ WAV ì €ì¥ ì œê±° (í†µì˜¤ë””ì˜¤ë§Œ ì €ì¥í•  ê²ƒ)
      // audioFolder?.file(
      //   `scene_${sceneNum}.wav`,
      //   audioBufferToWav(scene.audioBuffer)
      // );

      cursor = end;
      setZipProgress(Math.round(((i + 1) / total) * 100));
    }

    // âœ… í†µ ìë§‰ 1ê°œë§Œ ì €ì¥
    srtFolder?.file(
      "full_subtitles.srt",
      srtEntries.join('\n')
    );

    // âœ… í†µ ì˜¤ë””ì˜¤ 1ê°œë§Œ ì €ì¥ (fullSpeechAudioBuffer ì‚¬ìš©)
    if (fullSpeechAudioBuffer) {
      audioFolder?.file(
        "full_audio.wav",
        audioBufferToWav(fullSpeechAudioBuffer)
      );
    }

    const content = await zip.generateAsync({ type: "blob" });
    const a = document.createElement("a");
    a.href = URL.createObjectURL(content);
    a.download = `${state.metadata?.title || 'assets'}_studio.zip`;
    a.click();
  } finally {
    setIsZipping(false);
    setZipProgress(0);
  }
};



 const stopPreview = () => {
  // âœ… í˜„ì¬ ì¬ìƒ â€œì„¸ì…˜â€ ë¬´íš¨í™” (ì§„ì§œ í•µì‹¬)
  playTokenRef.current += 1;

  setIsPlaying(false);

  if (requestRef.current) {
    cancelAnimationFrame(requestRef.current);
    requestRef.current = 0;
  }

  // âœ… state(currentTime)ëŠ” í•œ í”„ë ˆì„ ëŠ¦ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ref ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
  previewTimeRef.current = currentTimeRef.current;

  if (audioSourceNodeRef.current) {
    try { audioSourceNodeRef.current.stop(); } catch {}
    audioSourceNodeRef.current = null;
  }

  if (bgmAudioRef.current) {
    try { bgmAudioRef.current.pause(); } catch {}
  }

  try {
    videoARef.current?.pause();
    videoBRef.current?.pause();
  } catch {}
};



  const startAudioAtTime = async (time: number) => {
    if (!fullSpeechAudioBuffer) return;

    if (audioSourceNodeRef.current) {
      try { audioSourceNodeRef.current.stop(); } catch {}
      audioSourceNodeRef.current = null;
    }

    if (!audioCtxRef.current || audioCtxRef.current.state === 'closed') {
      audioCtxRef.current = new AudioContext({ sampleRate: AUDIO_SR });
    }

if (audioCtxRef.current.state === 'suspended') {
  await audioCtxRef.current.resume();
}

    const currentEntry = sceneTimeline.find(e => time >= e.start && time < e.end);

    if (currentEntry && !currentEntry.scene.isHeader && currentEntry.audioOffsetInFullBuffer !== undefined) {
      const source = audioCtxRef.current.createBufferSource();
      source.buffer = fullSpeechAudioBuffer;

      const gainNode = audioCtxRef.current.createGain();
      gainNode.gain.value = 0.95;

      source.connect(gainNode);
      gainNode.connect(audioCtxRef.current.destination);

      const finalOffset = Math.max(0, currentEntry.audioOffsetInFullBuffer + (time - currentEntry.start));
      if (finalOffset < fullSpeechAudioBuffer.duration) {
        source.start(0, finalOffset);
        audioSourceNodeRef.current = source;
      }
    }

    if (bgmAudioRef.current && bgmAudioRef.current.readyState >= 2) {
      const bgmDuration = bgmAudioRef.current.duration;
      if (bgmDuration > 0) {
        bgmAudioRef.current.currentTime = time % bgmDuration;
        bgmAudioRef.current.play().catch(() => {});
      }
    }
  };
const togglePreview = async () => {
  if (stats.readyAudio === 0) return;

if (isPlaying) {
  stopPreview();

  // ğŸ”¥ ì˜¤ë””ì˜¤ í¸ì§‘ ëª¨ë“œ ê°•ì œ í•´ì œ
  setIsBreathEditing(false);
  setActiveBreathKey(null);
  setBreathEditText("");
  breathEditTextRef.current = "";

  return;
}

  if (isBreathEditing) {
    setIsBreathEditing(false);
    setActiveBreathKey(null);
    setBreathEditText("");
    breathEditTextRef.current = "";
  }
 const startTime =
  currentTimeRef.current >= totalDuration - 0.1
    ? totalDuration
    : currentTimeRef.current;


  // âœ… ìƒˆ ì¬ìƒ ì„¸ì…˜ í† í° ë°œê¸‰
  const myToken = (playTokenRef.current += 1);

  previewTimeRef.current = startTime;
  currentTimeRef.current = startTime;
  setCurrentTime(startTime);

  await startAudioAtTime(startTime);
  setIsPlaying(true);

  let last = performance.now();

  const frame = (now: number) => {
    // âœ… stopPreview()ê°€ ë¶ˆë¦¬ë©´ í† í°ì´ ë°”ë€Œë¯€ë¡œ ì¦‰ì‹œ ì¤‘ë‹¨
    if (playTokenRef.current !== myToken) return;

    const dt = (now - last) / 1000;
    last = now;

    const nextTime = previewTimeRef.current + dt;
    previewTimeRef.current = nextTime;

    if (nextTime >= totalDuration) {
      previewTimeRef.current = totalDuration;
      currentTimeRef.current = totalDuration;
      setCurrentTime(totalDuration);
      stopPreview();
      return;
    }

    currentTimeRef.current = nextTime;
    setCurrentTime(nextTime);
    requestRef.current = requestAnimationFrame(frame);
  };

  requestRef.current = requestAnimationFrame(frame);
};

const deleteScene = (id: string) => {
  pushToHistory();

  const scenes = stateRef.current.scenes;
  const idx = scenes.findIndex(s => s.id === id);
  if (idx === -1) return;

  const nextScenes = scenes.filter(s => s.id !== id);

  setState(p => ({ ...p, scenes: nextScenes }));

  // ğŸ”¥ ì‚­ì œ í›„ ì„ íƒí•  ì”¬ ê³„ì‚° (ë°”ë¡œ ìœ„, ì—†ìœ¼ë©´ ì•„ë˜)
  let nextIndex = idx - 1;
  if (nextIndex < 0) nextIndex = 0;
  if (nextIndex >= nextScenes.length) nextIndex = nextScenes.length - 1;

  const nextScene = nextScenes[nextIndex];

  if (nextScene) {
    const entry = sceneTimeline.find(e => e.scene.id === nextScene.id);
    if (entry) {
      previewTimeRef.current = entry.start;
      currentTimeRef.current = entry.start;
      setCurrentTime(entry.start);
    }
  }

  setIsBreathEditing(false);
  setActiveBreathKey(null);
  setBreathEditText("");
  breathEditTextRef.current = "";
};

const handleAddScene = () => {
  pushToHistory();

  const targetScene = currentScene || state.scenes[state.scenes.length - 1];
  if (!targetScene) return;

  const index = state.scenes.findIndex(s => s.id === targetScene.id);
  if (index === -1) return;

  const newScene: Scene = {
    id: `s-man-${Date.now()}`,
    visualGroupId: targetScene.visualGroupId || 'man',
    visualPrompt: '',
    subtitle: 'ìƒˆ ì¥ë©´ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.',
    status: 'pending',
    isHeader: false,
    estimatedDurationSeconds: 5
  };

  const newScenes = [...state.scenes];
  newScenes.splice(index + 1, 0, newScene);

  setState(p => ({ ...p, scenes: newScenes }));

  // ğŸ”¥ ìƒˆë¡œ ë§Œë“  ì”¬ì„ ì¦‰ì‹œ ì„ íƒ
  requestAnimationFrame(() => {
    const newIndex = index + 1;
    const t = sceneTimeline[newIndex]?.start ?? 0;
    handleSeek(t);
  });
};


const handleSeek = async (time: number) => {
  // ğŸ”¥ ë¬¶ìŒ í¸ì§‘ ê°•ì œ í•´ì œ
  if (isBreathEditing) {
    setIsBreathEditing(false);
    setActiveBreathKey(null);
    setBreathEditText("");
    breathEditTextRef.current = "";
  }

  previewTimeRef.current = time;
  setIsBreathEditing(false);
setActiveBreathKey(null);
setBreathEditText("");
breathEditTextRef.current = "";
  currentTimeRef.current = time;
  setCurrentTime(time);
  setIsBreathEditing(false);

  if (isPlaying) {
    await startAudioAtTime(time);
  }
};






  const handleAddHeader = () => {
     pushToHistory(); 
    const targetScene = currentScene || state.scenes[state.scenes.length - 1];
    if (!targetScene) return;

    const index = state.scenes.findIndex(s => s.id === targetScene.id);
    if (index === -1) return;

    const newHeader: Scene = {
      id: `h-man-${Date.now()}`,
      visualGroupId: 'header',
      visualPrompt: 'ì†Œì œëª©',
      subtitle: '[ìƒˆ ì†Œì œëª© ì…ë ¥]',
      status: 'completed',
      isHeader: true,
      imageUrl: `data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxyZWN0IHdpZHRoPSIxIiBoZWlnaHQ9IjEiIGZpbGw9InRyYW5zcGFyZW50Ii8+PC9zdmc+`
    };

    const newScenes = [...state.scenes];
    newScenes.splice(index + 1, 0, newHeader);
    setState(p => ({ ...p, scenes: newScenes }));
  };

  return (
    <div className="min-h-screen bg-black text-zinc-100 flex flex-col overflow-hidden">
      {viewMode === 'setup' ? (
        <>
          <Header />
          <div className="h-10 flex-shrink-0" />
       <main className="w-full mx-auto px-24 py-6 flex flex-col gap-4">




         {/* ğŸ›ï¸ ì˜ìƒ / ìŒì„± ì„¤ì • */}
<section className="bg-zinc-900 border border-zinc-800 rounded-3xl p-6 space-y-6">
  <VideoSettingsPanel
    settings={state.videoSettings}
    onChange={(u) => setState(p => ({ ...p, videoSettings: { ...p.videoSettings, ...u } }))}
    disabled={state.isAnalyzing}
  />

  <VoiceSelector
    selectedVoice={state.selectedVoice}
    onSelect={v => setState(p => ({ ...p, selectedVoice: v }))}
    voiceSpeed={state.voiceSpeed}
    onSpeedChange={s => setState(p => ({ ...p, voiceSpeed: s }))}
    voicePitch={state.voicePitch}
    onPitchChange={v => setState(p => ({ ...p, voicePitch: v }))}
    selectedBgm={state.bgmUrl}
    onSelectBgm={u => setState(p => ({ ...p, bgmUrl: u }))}
    bgmVolume={state.bgmVolume}
    onBgmVolumeChange={v => setState(p => ({ ...p, bgmVolume: v }))}
    disabled={state.isAnalyzing}
  />
</section>

{/* ğŸ¨ ìŠ¤íƒ€ì¼ / ì°¸ì¡° ì´ë¯¸ì§€ */}
<section className="bg-zinc-900 border border-zinc-800 rounded-3xl p-6 space-y-6">
  <StyleSelector
    selectedStyle={state.selectedStyle}
    onSelect={s => setState(p => ({ ...p, selectedStyle: s }))}
    referenceImage={state.referenceImage}
    onReferenceImageChange={img => setState(p => ({ ...p, referenceImage: img }))}
    disabled={state.isAnalyzing}
  />

  <AdvancedSettings
    characterPrompt={state.characterPrompt}
    onCharacterPromptChange={v => setState(p => ({ ...p, characterPrompt: v }))}
    atmospherePrompt={state.atmospherePrompt}
    onAtmospherePromptChange={v => setState(p => ({ ...p, atmospherePrompt: v }))}
    disabled={state.isAnalyzing}
  />
</section>

{/* ğŸ“¦ ì‚¬ìš©ì ì—ì…‹ */}
<section className="bg-zinc-900 border border-zinc-800 rounded-3xl p-6 space-y-6">
  <AssetLibrary
    assets={state.userAssets}
    onAssetsChange={a => setState(p => ({ ...p, userAssets: a }))}
    skipInitialImageGen={state.skipInitialImageGen}
    onSkipInitialImageGenChange={v => setState(p => ({ ...p, skipInitialImageGen: v }))}
    disabled={state.isAnalyzing}
  />
</section>

           <section className="bg-zinc-900 border border-zinc-800 rounded-3xl p-6 pointer-events-auto">

<ScriptInput
  value={state.script}
  onChange={(t) => setState(p => ({ ...p, script: t }))}
  disabled={state.isAnalyzing}
  mode="script"
  onModeChange={() => {}}
  speed={state.voiceSpeed}
/>

         <button
  type="button"
  disabled={state.isAnalyzing}
  onClick={() => setIsConfirmOpen(true)}
  className={`w-full mt-6 h-16 rounded-xl font-black text-2xl flex items-center justify-center gap-3 shadow-xl transition-all ${
    state.isAnalyzing
      ? "bg-yellow-400 text-black cursor-wait"
      : "bg-yellow-400 text-black active:scale-[0.98]"
  }`}
>
  {state.isAnalyzing ? (
    <>
      <Loader2 className="w-8 h-8 animate-spin" />
      <span>ëŒ€ë³¸ ë¶„ì„ì¤‘...</span>
    </>
  ) : (
    <>
      <Sparkles className="w-8 h-8" />
      <span>ì‘ì—… ì‹œì‘í•˜ê¸°</span>
    </>
  )}
</button>





            </section>
          </main>

          {/* í˜•ë‹˜ ìš”ì²­: ìµœì¢… ì„¤ì • í™•ì¸ì°½ UI ì—…ë°ì´íŠ¸ */}
          {isConfirmOpen && (
            <div className="fixed inset-0 z-[100] flex items-center justify-center p-4 bg-black/80 backdrop-blur-sm animate-in fade-in">
              <div className="bg-zinc-900 border border-zinc-800 rounded-[2.5rem] w-full max-w-lg p-8 space-y-8 animate-in zoom-in-95">
                <div className="flex justify-between items-center">
                  <h3 className="text-2xl font-black text-white uppercase tracking-tighter">ìµœì¢… ì„¤ì • í™•ì¸</h3>
                  <button onClick={() => setIsConfirmOpen(false)} className="p-2 hover:bg-zinc-800 rounded-full text-zinc-500 transition-colors">
                    <X className="w-6 h-6" />
                  </button>
                </div>

                <div className="grid grid-cols-2 gap-4">
                  <div className="bg-black/40 p-4 rounded-2xl border border-zinc-800/50 space-y-1">
                    <span className="text-[10px] font-black text-zinc-500 uppercase tracking-widest flex items-center gap-1.5">
                      <Monitor className="w-3 h-3" /> í™”ë©´ ë¹„ìœ¨
                    </span>
                    <p className="text-lg font-black text-yellow-400">{state.videoSettings.aspectRatio}</p>
                  </div>

                  <div className="bg-black/40 p-4 rounded-2xl border border-zinc-800/50 space-y-1">
                    <span className="text-[10px] font-black text-zinc-500 uppercase tracking-widest flex items-center gap-1.5">
                      <Mic2 className="w-3 h-3" /> ì„ íƒí•œ ì„±ìš°
                    </span>
                    <p className="text-lg font-black text-emerald-400">
                      {VOICE_LABEL_MAP[state.selectedVoice] ?? state.selectedVoice}
                    </p>
                  </div>

                  <div className="bg-black/40 p-4 rounded-2xl border border-zinc-800/50 space-y-1">
                    <span className="text-[10px] font-black text-zinc-500 uppercase tracking-widest flex items-center gap-1.5">
                      <Music className="w-3 h-3" /> ì‚¬ìš©ì ì—ì…‹
                    </span>
                    <p className="text-lg font-black text-blue-400">{state.userAssets.length}ê°œ ì—…ë¡œë“œë¨</p>
                  </div>

                  <div className="bg-black/40 p-4 rounded-2xl border border-zinc-800/50 space-y-1">
                    <span className="text-[10px] font-black text-zinc-500 uppercase tracking-widest flex items-center gap-1.5">
                      <ImageIcon className="w-3 h-3" /> ì°¸ì¡° ì´ë¯¸ì§€
                    </span>
                    <p className="text-lg font-black text-rose-400">{state.referenceImage ? 'ì‚¬ìš© ì¤‘' : 'ë¯¸ì‚¬ìš©'}</p>
                  </div>
                </div>

                <div className="bg-yellow-400/5 p-4 rounded-2xl border border-yellow-400/20 text-center">
                  <p className="text-sm text-yellow-400/80 font-bold leading-relaxed">
                    {state.skipInitialImageGen
                      ? "AI ìƒì„± ë° ì—ì…‹ ìë™ ë°°ì¹˜ë¥¼ ìƒëµí•˜ê³ , ì˜¤ë””ì˜¤ë§Œ ìƒì„±í•˜ì—¬ ì‹œì‘í•©ë‹ˆë‹¤."
                      : "ì‚¬ìš©ì ì—ì…‹ ìš°ì„  ë°°ì¹˜ í›„ ë‚˜ë¨¸ì§€ëŠ” AIê°€ ìë™ ìƒì„±í•©ë‹ˆë‹¤."}
                  </p>
                </div>

                <div className="flex gap-4">
                  <button onClick={() => setIsConfirmOpen(false)} className="flex-1 py-4 rounded-2xl font-black text-zinc-500 hover:text-white transition-colors">
                    ì·¨ì†Œí•˜ê¸°
                  </button>
                  <button
  onClick={handleStartAnalysis}
  disabled={state.isAnalyzing}
  className={`flex-[2] py-4 rounded-2xl font-black text-xl shadow-lg transition-all flex items-center justify-center gap-2 ${
    state.isAnalyzing
      ? "bg-zinc-600 text-zinc-300 cursor-not-allowed"
      : "bg-yellow-400 text-black active:scale-95"
  }`}
>
  {state.isAnalyzing ? (
    <>
      <Loader2 className="w-5 h-5 animate-spin" />
      <span>ëŒ€ë³¸ ë¶„ì„ì¤‘...</span>
    </>
  ) : (
    <>
      <Sparkles className="w-5 h-5" />
      <span>ì§€ê¸ˆ ë¶„ì„ ì‹œì‘</span>
    </>
  )}
</button>

                </div>
              </div>
            </div>
          )}
        </>
      ) : (
        <div className="min-h-screen bg-black flex flex-col overflow-hidden">
              <header className="h-20 border-b border-zinc-800 bg-zinc-950 flex items-center justify-between px-6 flex-shrink-0">
            <div className="flex items-center gap-4 overflow-hidden">
              <button
                onClick={() => window.location.href = "/"}
                className="flex items-center gap-2 hover:opacity-80"
              >
                <img src={import.meta.env.BASE_URL + "logo.png"} className="w-7 h-7" />
                <span className="font-black text-sm">ë…¸ê¹¡ STUDIO</span>
              </button>

              <button
                onClick={() => setViewMode('setup')}
                className="p-2 hover:bg-zinc-800 rounded-lg flex-shrink-0"
              >
                <ChevronLeft />
              </button>

              <h2 className="text-sm font-black text-zinc-400 truncate max-w-[200px] flex-shrink-0">
                {state.metadata?.title}
              </h2>

              <div className="h-10 border-l border-zinc-800 ml-2 pl-4 flex items-center flex-shrink-0">
                <BgmSelector
                  variant="compact"

                  selectedBgm={state.bgmUrl}
                  onSelect={u => setState(p => ({ ...p, bgmUrl: u }))}
                  volume={state.bgmVolume}
                  onVolumeChange={v => setState(p => ({ ...p, bgmVolume: v }))}
                  disabled={isGenerating}
                />
              </div>
            </div>

            <div className="flex items-center gap-3">
              <button
  onClick={undo}
  disabled={historyCount === 0}
  className={`p-2 rounded-lg border ${
    historyCount === 0
      ? 'border-zinc-800 opacity-30 cursor-not-allowed'
      : 'border-zinc-700 hover:bg-zinc-800'
  }`}
  title="ë’¤ë¡œê°€ê¸° (Ctrl+Z)"
>
  <Undo2 className="w-4 h-4" />
</button>

<button
  onClick={redo}
  disabled={redoCount === 0}
  className={`p-2 rounded-lg border ${
    redoCount === 0
      ? 'border-zinc-800 opacity-30 cursor-not-allowed'
      : 'border-zinc-700 hover:bg-zinc-800'
  }`}
  title="ì•ìœ¼ë¡œê°€ê¸° (Ctrl+Shift+Z)"
>
  <Redo2 className="w-4 h-4" />
</button>

              <button onClick={handleAddScene} className="px-3 py-1.5 bg-zinc-800 text-zinc-300 border border-zinc-700 rounded-lg text-[11px] font-black hover:bg-zinc-700 active:scale-95 flex items-center gap-1.5">
                <Plus className="w-3 h-3" />ì¥ë©´ ì¶”ê°€
              </button>
              <button onClick={handleAddHeader} className="px-3 py-1.5 bg-zinc-800 text-zinc-300 border border-zinc-700 rounded-lg text-[11px] font-black hover:bg-zinc-700 active:scale-95 flex items-center gap-1.5">
                <Type className="w-3 h-3" />ì†Œì œëª© ì¶”ê°€
              </button>
{(stats.hasMissingImage || stats.hasError) && (
  <button
    onClick={() => processFullBatch('image')}
    disabled={isImageBatchRunningState}
    className="px-3 py-1.5 bg-yellow-400 text-black rounded-lg text-[11px] font-black hover:bg-yellow-300 shadow-md flex items-center gap-1.5 active:scale-95"
  >

    {isImageBatchRunningState ? (
      <Loader2 className="w-3 h-3 animate-spin" />
    ) : (
      <Wand2 className="w-3 h-3" />
    )}
    ë¹ˆ ì¹¸ ì±„ìš°ê¸°
  </button>
)}

              <div className="flex flex-col items-end flex-shrink-0">
                <span className="text-[10px] font-bold text-zinc-500 uppercase flex items-center gap-2">
                  {stats.isAllReady ? <CheckCircle2 className="text-emerald-500 w-3 h-3" /> : <Loader2 className="animate-spin text-yellow-400 w-3 h-3" />}
                  {stats.isAllReady ? "ì¤€ë¹„ ì™„ë£Œ" : `${stats.progress}%`}
                </span>
                <div className="w-24 h-1.5 bg-zinc-800 rounded-full mt-1.5 overflow-hidden">
                  <div className="h-full bg-yellow-400 transition-all duration-300" style={{ width: `${stats.progress}%` }} />
                </div>
              </div>
            </div>
          </header>

         <main className="flex-grow flex flex-col lg:flex-row p-6 gap-6 h-[calc(100vh-80px)] min-h-0">

           <div className="lg:w-1/2 flex flex-col gap-4 min-h-0">
              <div className="flex-grow flex items-center justify-center bg-zinc-950/30 rounded-3xl border border-zinc-800/50 p-4 min-h-0">
                <div
                  id="preview-container"
                  className="bg-black border border-zinc-800 rounded-2xl overflow-hidden relative shadow-2xl [container-type:inline-size]"
                  style={{
                    aspectRatio: state.videoSettings.aspectRatio === '16:9' ? '16/9' : '9/16',
                    maxHeight: '100%',
                    maxWidth: '100%',
                    height: state.videoSettings.aspectRatio === '9:16' ? '100%' : 'auto',
                    width: state.videoSettings.aspectRatio === '16:9' ? '100%' : 'auto'
                  }}
                >
          {currentScene?.imageUrl && currentScene.userAssetType === 'video' && (
  <>
 <video
  ref={videoARef}
  className="absolute inset-0 w-full h-full object-cover bg-transparent"
  muted
  playsInline
  preload="auto"
  style={{ opacity: activeVideo === 'A' ? 1 : 0 }}
/>

<video
  ref={videoBRef}
  className="absolute inset-0 w-full h-full object-cover bg-transparent"
  muted
  playsInline
  preload="auto"
  style={{ opacity: activeVideo === 'B' ? 1 : 0 }}
/>



  </>
)}


{currentScene?.imageUrl && currentScene.userAssetType !== 'video' && (
  <img
    src={currentScene.imageUrl}
    className="w-full h-full object-cover"
    style={{ transform: `scale(${currentZoomScale})` }}
  />
)}




                  <div className="absolute inset-0 pointer-events-none flex flex-col items-center justify-center z-0">

                    {currentHeader && (
                      <div className="absolute top-[4%] left-0 right-0 text-center">
                        <span
                          className="px-3 py-1 text-white/90 font-black drop-shadow-lg"
                          style={{ fontSize: state.videoSettings.aspectRatio === '16:9' ? '2.8cqw' : '5.0cqw' }}
                        >
                          {currentHeader}
                        </span>
                      </div>
                    )}

                    {!currentScene?.isHeader && subtitleLines.length > 0 && (
                      <div
                        className="absolute left-0 right-0 px-[2.5%] text-center font-black flex flex-col items-center justify-end break-keep"
                        style={{
                          bottom: `${state.videoSettings.subtitlePosition}%`,
                        
                        }}
                      >
                       <div className="relative inline-block">
  {state.videoSettings.showSubtitleBox && (
    <div className="absolute inset-x-[-4%] inset-y-[-5%] bg-black/70 rounded-[2px] z-0" />
  )}
  <div className="relative z-10">
   {subtitleLines.map((line, idx) => {
  const ratioConfig = SUBTITLE_RATIOS[state.videoSettings.aspectRatio as '16:9' | '9:16'];
  const M_ONLY_SCALE = 0.90;

  const base = previewWidth > 0
    ? previewWidth * ratioConfig[state.videoSettings.subtitleSize as SubtitleSize]
    : 16;

  const fontSize =
    state.videoSettings.subtitleSize === 'M'
      ? base * M_ONLY_SCALE
      : base;

  return (
<span
  key={idx}
  className="block text-white"
  style={{
    fontSize: `${fontSize}px`,
    lineHeight: 1.45,
    whiteSpace: 'pre-wrap',
    wordBreak: 'break-word',
    overflowWrap: 'anywhere',
    textShadow: '0 2px 4px rgba(0,0,0,0.5)'
  }}
>
  {line}
</span>

  );
})}


                          </div>
                        </div>
                      </div>
                    )}
                  </div>
                </div>
              </div>

              <div className="bg-zinc-900/80 p-4 rounded-2xl border border-zinc-800 flex items-center gap-4 flex-shrink-0">
               <button
  onClick={() => {
    setIsBreathEditing(false);
    setActiveBreathKey(null);
    setBreathEditText("");
    breathEditTextRef.current = "";
    togglePreview();
  }}
  className="p-4 rounded-full bg-yellow-400 text-black active:scale-90 transition-all"
>
                  {isPlaying ? <StopCircle /> : <PlayCircle />}
                </button>
                <input
                  type="range"
                  min="0"
                  max={totalDuration}
                  step="0.1"
                  value={currentTime}
                  onChange={e => handleSeek(parseFloat(e.target.value))}
                  className="flex-grow accent-yellow-400"
                />
                <span className="text-xs font-mono text-zinc-400">{formatTime(currentTime)} / {formatTime(totalDuration)}</span>
              </div>

             <div className="flex-shrink-0 relative z-50">
  <VideoExporter
    scenes={state.scenes}
    settings={state.videoSettings}
    bgmUrl={state.bgmUrl}
    bgmVolume={state.bgmVolume}
    metadata={state.metadata}
    disabled={isGenerating}
    fullSpeechAudioBuffer={fullSpeechAudioBuffer}
    sceneTimeline={sceneTimeline}
    assetStatus={stats.isAllReady ? 'ready' : 'pending'}
    isExporting={isExporting}
    setIsExporting={setIsExporting}
    onDownloadAllAssets={handleDownloadAllAssets}
    isZipping={isZipping}
    zipProgress={zipProgress}
    exportProgress={exportProgress}
    setExportProgress={setExportProgress}
  />
</div>

            </div>

<div
  className={`lg:w-1/2 flex flex-col gap-2 h-full pr-1 transition-all ${
    isExporting ? 'pointer-events-none relative z-0' : 'relative z-10'
  }`}
>




{(() => {
  // âœ… breathId ê¸°ì¤€ ê·¸ë£¹ ë§Œë“¤ê¸° (headerëŠ” ë³„ë„)
  const breathGroups = new Map<string, Scene[]>();
  const headerGroups = new Map<string, Scene[]>();

  state.scenes.forEach(sc => {
    if (sc.isHeader) {
      headerGroups.set(`h-${sc.id}`, [sc]);
      return;
    }
    const key = String((sc as any).breathId ?? "");
    if (!breathGroups.has(key)) breathGroups.set(key, []);
    breathGroups.get(key)!.push(sc);
  });

  const renderedBreath = new Set<string>();

  // âœ… í™”ë©´ì—ëŠ” "ì›ë˜ ì”¬ ìˆœì„œ"ëŒ€ë¡œ ë Œë”
  return state.scenes.map((s) => {
    // header
    if (s.isHeader) {
      const i = state.scenes.findIndex(x => x.id === s.id);
      return (
        <SceneCard
          key={s.id}
          ref={el => (sceneRefs.current[s.id] = el)}
          scene={s}
          index={i}
          metadata={state.metadata}
          getBreathKey={() => null}
          onDeleteScene={deleteScene}
          onRegenerateBreathGroup={(key) => {
            setActiveBreathKey(key);
            processBreathGroupAudio(key);
          }}
          onEnterAudioEditMode={(key) => {
  setActiveBreathKey(key);

  const groupScenes = state.scenes.filter(
    x => !x.isHeader && String((x as any).breathId) === key
  );

  // ì´ë¯¸ ì”¬ë“¤ì´ ë™ì¼í•œ ìë§‰ìœ¼ë¡œ ë§ê°€ì ¸ ìˆìœ¼ë©´ 1ê°œë§Œ ì‚¬ìš©
  const unique = Array.from(new Set(groupScenes.map(s => s.subtitle.trim())));

  const groupText =
    unique.length === 1
      ? unique[0]
      : unique.join("\n");

  setBreathEditText(groupText);
  setIsBreathEditing(true);
}}

          onExitAudioEditMode={() => {
            setActiveBreathKey(null);
            setIsBreathEditing(false);
            setBreathEditText("");
          }}
          onRegenerateImage={(id, p) => {
            updateVisualPrompt(id, p || s.visualPrompt);
            processSingleAsset(id, "image");
          }}
          onUpdateSubtitle={updateSubtitle}
          onRegenerateAudio={(id, sub) => {
            updateSubtitle(id, sub || s.subtitle);
            processSingleAsset(id, "audio");
          }}
          onUpdateVisualPrompt={updateVisualPrompt}
          onClick={() => {
            setIsBreathEditing(false);
            setActiveBreathKey(null);
            setBreathEditText("");
            handleSeek(sceneTimeline[i].start);
          }}
          // âœ… ê¸°ë³¸ ìƒíƒœì—ì„œë§Œ ê°œë³„ ë…¸ë€í…Œë‘ë¦¬
          isActive={!isBreathEditing && currentScene?.id === s.id}
          onRetry={(id) => processSingleAsset(id, "image")}
          onImageUpload={handleManualImageUpload}
          skipInitialImageGen={state.skipInitialImageGen}

          activeBreathKey={activeBreathKey}
          breathEditText={breathEditText}
          onBreathEditTextChange={(key, text) => setBreathEditText(text)}
          onUpdateBreathGroupSubtitle={updateBreathGroupSubtitle}
        />
      );
    }

    const breathKey = String((s as any).breathId ?? "");
    const isActiveBreath = isBreathEditing && activeBreathKey && breathKey === activeBreathKey;

    // âœ… í¸ì§‘ ëª¨ë“œê°€ ì•„ë‹ˆë©´: ë¬¶ìŒ wrapper ì—†ì´ "ì”¬ ì¹´ë“œ 1ê°œ"ë§Œ
    if (!isBreathEditing) {
      const i = state.scenes.findIndex(x => x.id === s.id);
      return (
        <SceneCard
          key={s.id}
          ref={el => (sceneRefs.current[s.id] = el)}
          scene={s}
          index={i}
          metadata={state.metadata}
          getBreathKey={() => (!s.isHeader ? String((s as any).breathId ?? "") : null)}
          onDeleteScene={deleteScene}
          onRegenerateBreathGroup={(key) => {
            setActiveBreathKey(key);
            processBreathGroupAudio(key);
          }}
          onEnterAudioEditMode={(key) => {
  setActiveBreathKey(key);

  const groupScenes = state.scenes.filter(
    x => !x.isHeader && String((x as any).breathId) === key
  );

  // ì´ë¯¸ ì”¬ë“¤ì´ ë™ì¼í•œ ìë§‰ìœ¼ë¡œ ë§ê°€ì ¸ ìˆìœ¼ë©´ 1ê°œë§Œ ì‚¬ìš©
  const unique = Array.from(new Set(groupScenes.map(s => s.subtitle.trim())));

  const groupText =
    unique.length === 1
      ? unique[0]
      : unique.join("\n");

  setBreathEditText(groupText);
  setIsBreathEditing(true);
}}

          onExitAudioEditMode={() => {
            setActiveBreathKey(null);
            setIsBreathEditing(false);
            setBreathEditText("");
          }}
          onRegenerateImage={(id, p) => {
            updateVisualPrompt(id, p || s.visualPrompt);
            processSingleAsset(id, "image");
          }}
          onUpdateSubtitle={updateSubtitle}
          onRegenerateAudio={(id, sub) => {
            updateSubtitle(id, sub || s.subtitle);
            processSingleAsset(id, "audio");
          }}
          onUpdateVisualPrompt={updateVisualPrompt}
          onClick={() => {
            setIsBreathEditing(false);
            setActiveBreathKey(null);
            setBreathEditText("");
            handleSeek(sceneTimeline[i].start);
          }}
          isActive={currentScene?.id === s.id}
          onRetry={(id) => processSingleAsset(id, "image")}
          onImageUpload={handleManualImageUpload}
          skipInitialImageGen={state.skipInitialImageGen}

          activeBreathKey={activeBreathKey}
          breathEditText={breathEditText}
          onBreathEditTextChange={(key, text) => setBreathEditText(text)}
          onUpdateBreathGroupSubtitle={updateBreathGroupSubtitle}
        />
      );
    }

    // âœ… í¸ì§‘ ëª¨ë“œì¼ ë•Œ:
    // - activeBreathKey ë¬¶ìŒë§Œ "í° ë„¤ëª¨ wrapper"ë¡œ ê°ì‹¸ì„œ 1ë²ˆë§Œ ë Œë”
    // - ë‚˜ë¨¸ì§€ëŠ” í‰ì†Œì²˜ëŸ¼ ê°œë³„ ì”¬ ì¹´ë“œ
    if (!isActiveBreath) {
      const i = state.scenes.findIndex(x => x.id === s.id);
      return (
        <SceneCard
          key={s.id}
          ref={el => (sceneRefs.current[s.id] = el)}
          scene={s}
          index={i}
          metadata={state.metadata}
          getBreathKey={() => (!s.isHeader ? String((s as any).breathId ?? "") : null)}
            onDeleteScene={deleteScene}
          onRegenerateBreathGroup={(key) => {
            setActiveBreathKey(key);
            processBreathGroupAudio(key);
          }}
          onEnterAudioEditMode={(key) => {
  setActiveBreathKey(key);

  const groupScenes = state.scenes.filter(
    x => !x.isHeader && String((x as any).breathId) === key
  );

  // ì´ë¯¸ ì”¬ë“¤ì´ ë™ì¼í•œ ìë§‰ìœ¼ë¡œ ë§ê°€ì ¸ ìˆìœ¼ë©´ 1ê°œë§Œ ì‚¬ìš©
  const unique = Array.from(new Set(groupScenes.map(s => s.subtitle.trim())));

  const groupText =
    unique.length === 1
      ? unique[0]
      : unique.join("\n");

  setBreathEditText(groupText);
  setIsBreathEditing(true);
}}

          onExitAudioEditMode={() => {
            setActiveBreathKey(null);
            setIsBreathEditing(false);
            setBreathEditText("");
          }}
          onRegenerateImage={(id, p) => {
            updateVisualPrompt(id, p || s.visualPrompt);
            processSingleAsset(id, "image");
          }}
          onUpdateSubtitle={updateSubtitle}
          onRegenerateAudio={(id, sub) => {
            updateSubtitle(id, sub || s.subtitle);
            processSingleAsset(id, "audio");
          }}
          onUpdateVisualPrompt={updateVisualPrompt}
          onClick={() => {
  // ğŸ”¥ ë¬¶ìŒ í¸ì§‘ ì™„ì „ í•´ì œ
  setIsBreathEditing(false);
  setActiveBreathKey(null);
  setBreathEditText("");
  breathEditTextRef.current = "";

  // ğŸ”¥ í•´ë‹¹ ì”¬ë§Œ í™œì„±í™”ë˜ë„ë¡ íƒ€ì„ë¼ì¸ ì´ë™
  const i2 = state.scenes.findIndex(x => x.id === s.id);
  handleSeek(sceneTimeline[i2].start);
}}

          // âœ… í¸ì§‘ ëª¨ë“œì—ì„œëŠ” ê°œë³„ ë…¸ë€í…Œë‘ë¦¬ ê¸ˆì§€
          isActive={false}
          onRetry={(id) => processSingleAsset(id, "image")}
          onImageUpload={handleManualImageUpload}
          skipInitialImageGen={state.skipInitialImageGen}

          activeBreathKey={activeBreathKey}
          breathEditText={breathEditText}
          onBreathEditTextChange={(key, text) => setBreathEditText(text)}
          onUpdateBreathGroupSubtitle={updateBreathGroupSubtitle}
        />
      );
    }

    // âœ… active ë¬¶ìŒ wrapperëŠ” 1ë²ˆë§Œ
    if (renderedBreath.has(breathKey)) return null;
    renderedBreath.add(breathKey);

    const groupScenes = breathGroups.get(breathKey) ?? [];

    return (
      <div key={`breath-wrap-${breathKey}`} className="relative">
        {/* âœ… ë ˆì´ì•„ì›ƒ ì•ˆ ë°€ë¦¬ëŠ” 'ì˜¤ë²„ë ˆì´ í…Œë‘ë¦¬' */}
        <div className="pointer-events-none absolute inset-0 rounded-2xl border-2 border-yellow-400 z-10" />

        <div className="space-y-2">
          {groupScenes.map(gs => {
            const i = state.scenes.findIndex(x => x.id === gs.id);
            return (
              <SceneCard
                key={gs.id}
                ref={el => (sceneRefs.current[gs.id] = el)}
                scene={gs}
                index={i}
                metadata={state.metadata}
                getBreathKey={() => (!gs.isHeader ? String((gs as any).breathId ?? "") : null)}
                onDeleteScene={deleteScene}
                onRegenerateBreathGroup={(key) => {
                  setActiveBreathKey(key);
                  processBreathGroupAudio(key);
                }}
               onEnterAudioEditMode={(key) => {
  setActiveBreathKey(key);

  const groupScenes = state.scenes.filter(
    x => !x.isHeader && String((x as any).breathId) === key
  );

  // ì´ë¯¸ ì”¬ë“¤ì´ ë™ì¼í•œ ìë§‰ìœ¼ë¡œ ë§ê°€ì ¸ ìˆìœ¼ë©´ 1ê°œë§Œ ì‚¬ìš©
  const unique = Array.from(new Set(groupScenes.map(s => s.subtitle.trim())));

  const groupText =
    unique.length === 1
      ? unique[0]
      : unique.join("\n");

  setBreathEditText(groupText);
  setIsBreathEditing(true);
}}

                onExitAudioEditMode={() => {
                  setActiveBreathKey(null);
                  setIsBreathEditing(false);
                  setBreathEditText("");
                }}
                onRegenerateImage={(id, p) => {
                  updateVisualPrompt(id, p || gs.visualPrompt);
                  processSingleAsset(id, "image");
                }}
                onUpdateSubtitle={updateSubtitle}
                onRegenerateAudio={(id, sub) => {
                  updateSubtitle(id, sub || gs.subtitle);
                  processSingleAsset(id, "audio");
                }}
                onUpdateVisualPrompt={updateVisualPrompt}
                onClick={() => {
                  // âœ… í¸ì§‘ ëª¨ë“œ ìœ ì§€ ì¤‘ í´ë¦­ì€ ì”¬ ì„ íƒë§Œ(í…Œë‘ë¦¬ëŠ” ë¬¶ìŒë§Œ)
                  handleSeek(sceneTimeline[i].start);
                }}
                // âœ… í¸ì§‘ ëª¨ë“œì—ì„œëŠ” ì”¬ë³„ ë…¸ë€í…Œë‘ë¦¬ ê¸ˆì§€
                isActive={false}
                onRetry={(id) => processSingleAsset(id, "image")}
                onImageUpload={handleManualImageUpload}
                skipInitialImageGen={state.skipInitialImageGen}

                activeBreathKey={activeBreathKey}
                breathEditText={breathEditText}
                onBreathEditTextChange={(key, text) => setBreathEditText(text)}
                onUpdateBreathGroupSubtitle={updateBreathGroupSubtitle}
              />
            );
          })}
        </div>
      </div>
    );
  });
})()}




</div>


          </main>
        </div>
      )}
    </div>
  );
};

export default App;
